{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython import display\n",
    "\n",
    "AUDIO_PATH = 'dataset/FMA/fma_small/'\n",
    "METADATA_PATH = 'dataset/FMA/fma_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple wrapper class for (1-channel) audio data\n",
    "# data is a 1-D NumPy array containing the data\n",
    "# rate is a number expressing the samples per second\n",
    "# == Modified from 554X class example code ==\n",
    "class Audio:\n",
    "    def __init__(self, data, rate, fn):\n",
    "        self.data = data\n",
    "        self.rate = rate\n",
    "        self.filename = fn.split(\"/\")[-1]\n",
    "    def play(self):\n",
    "        return display.Audio(self.data, rate=self.rate)\n",
    "    def plot_wave(self):\n",
    "        librosa.display.waveplot(self.data, sr=self.rate)\n",
    "    def create_spectrum(self, n_fft, hop_length):\n",
    "        # n_fft = int(self.rate / 20)\n",
    "        # hop_length = n_fft / 4\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(self.data, n_fft=n_fft)), ref=np.max)\n",
    "        return D\n",
    "    def create_melspectrum(self, n_fft, hop_length):\n",
    "        D = librosa.power_to_db(librosa.feature.melspectrogram(self.data, sr=self.rate, n_fft=n_fft, hop_length=hop_length), ref=np.max)\n",
    "        return D\n",
    "    def plot_spectrum(self, D, y_axis, hop_length):\n",
    "        librosa.display.specshow(D, y_axis=y_axis, x_axis='time', sr=self.rate, hop_length=hop_length)\n",
    "    @classmethod\n",
    "    def fromfile(cls, fn):\n",
    "        return cls(*librosa.load(fn, sr=None), fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tids_from_directory(audio_dir):\n",
    "    \"\"\"Get track IDs from the mp3s in a directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_dir : str\n",
    "        Path to the directory where the audio files are stored.\n",
    "    Returns\n",
    "    -------\n",
    "        A list of track IDs.\n",
    "    \"\"\"\n",
    "    tids = []\n",
    "    for _, dirnames, files in os.walk(audio_dir):\n",
    "        if dirnames == []:\n",
    "            tids.extend(int(file[:-4]) for file in files)\n",
    "    return tids\n",
    "\n",
    "\n",
    "def get_audio_path(audio_dir, track_id):\n",
    "    \"\"\"\n",
    "    Return the path to the mp3 given the directory where the audio is stored\n",
    "    and the track ID.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import utils\n",
    "    >>> AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    >>> utils.get_audio_path(AUDIO_DIR, 2)\n",
    "    '../data/fma_small/000/000002.mp3'\n",
    "    \"\"\"\n",
    "    tid_str = '{:06d}'.format(track_id)\n",
    "    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "dataset/FMA/fma_small/135/135054.mp3\n"
     ]
    }
   ],
   "source": [
    "tids = get_tids_from_directory(AUDIO_PATH)\n",
    "print(len(tids))\n",
    "print(get_audio_path(AUDIO_PATH, tids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 4)\n",
      "{'Hip-Hop': 1, 'Pop': 2, 'Folk': 3, 'Experimental': 4, 'Rock': 5, 'International': 6, 'Electronic': 7, 'Instrumental': 8}\n"
     ]
    }
   ],
   "source": [
    "# Load genres and metadata\n",
    "tracks = pd.read_csv(os.path.join(METADATA_PATH, \"tracks.csv\"), index_col=0, header=[0, 1])\n",
    "keep_cols = [('set', 'split'), ('set', 'subset'), ('track', 'genre_top')]\n",
    "\n",
    "df_all = tracks[keep_cols]\n",
    "df_all = df_all[df_all[('set', 'subset')] == 'small'] # only extract FMA_small metadata\n",
    "df_all['track_id'] = df_all.index\n",
    "print(df_all.shape)\n",
    "\n",
    "# Create dictionary of genres from unique genre labels\n",
    "unique_genres = df_all[('track', 'genre_top')].unique()\n",
    "dict_genres = { unique_genres[i] : i+1 for i in range(0, len(unique_genres)) } # i+1 because feels weird to have 0 as label\n",
    "print(dict_genres)\n",
    "\n",
    "# df_all.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process data into spectrogram and genre labels for model\n",
    "def setup_model_data(df):\n",
    "    genres = []\n",
    "    X_spect = np.empty((0, 640, 128))\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            count += 1\n",
    "            tid = int(row['track_id'])\n",
    "            genre = str(row[('track', 'genre_top')])\n",
    "            genres.append(dict_genres[genre])\n",
    "            \n",
    "            audio = Audio.fromfile(get_audio_path(AUDIO_PATH, tid))\n",
    "            spect = audio.create_melspectrum(2048, 1024)\n",
    "            spect = spect.T[:640, :]\n",
    "            X_spect = np.append(X_spect, [spect], axis=0)\n",
    "            if count % 100 == 0:\n",
    "                print(\"Currently processing: \", count)\n",
    "        except:\n",
    "            print(\"Couldn't process: \", count)\n",
    "            continue\n",
    "    y_labels = np.array(genres)\n",
    "    return X_spect, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split full data into train, validation, test data frames\n",
    "df_train = df_all[df_all[('set', 'split')]=='training']\n",
    "df_valid = df_all[df_all[('set', 'split')]=='validation']\n",
    "df_test = df_all[df_all[('set', 'split')]=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONLY RUN AND UNCOMMENT THIS IF YOU NEED TO PROCESS TRAIN DATA AGAIN. \n",
    "\n",
    "# X_test, y_test = setup_model_data(df_test)\n",
    "# np.savez('test_data', X_test, y_test)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "(800, 640, 128) (800,)\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN AND UNCOMMENT THIS IF YOU NEED TO PROCESS VALIDATION DATA AGAIN.\n",
    "\n",
    "# X_valid, y_valid = setup_model_data(df_valid)\n",
    "# np.savez('valid_data', X_valid, y_valid)\n",
    "# print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "Currently processing:  1700\n",
      "Currently processing:  1800\n",
      "Currently processing:  1900\n",
      "Currently processing:  2000\n",
      "Currently processing:  2100\n",
      "Currently processing:  2200\n",
      "Currently processing:  2300\n",
      "Currently processing:  2400\n",
      "Currently processing:  2500\n",
      "Currently processing:  2600\n",
      "Currently processing:  2700\n",
      "Currently processing:  2800\n",
      "Currently processing:  2900\n",
      "Currently processing:  3000\n",
      "Currently processing:  3100\n",
      "Currently processing:  3200\n",
      "Couldn't process:  3265\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN AND UNCOMMENT THIS IF YOU NEED TO PROCESS TRAINING DATA AGAIN.\n",
    "# WARNING: TAKES A VERY LONG TIME!!!\n",
    "\n",
    "X_train, y_train = setup_model_data(df_train)\n",
    "np.savez('train_arr', X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 4) (1600, 4) (1600, 4) (1600, 4)\n"
     ]
    }
   ],
   "source": [
    "# Batch training data because it's too big to be processed all at once\n",
    "\n",
    "def splitDataFrameIntoSmaller(df, chunkSize = 1600): \n",
    "    listOfDf = list()\n",
    "    numberChunks = len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n",
    "    return listOfDf\n",
    "\n",
    "listDf = splitDataFrameIntoSmaller(df_train)\n",
    "df1_train = listDf[0]\n",
    "df2_train = listDf[1]\n",
    "df3_train = listDf[2]\n",
    "df4_train = listDf[3]\n",
    "print(df1_train.shape, df2_train.shape, df3_train.shape, df4_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "(1600, 640, 128) (1600,)\n"
     ]
    }
   ],
   "source": [
    "X1_train, y1_train = setup_model_data(df1_train)\n",
    "np.savez('train1_arr', X1_train, y1_train)\n",
    "print(X1_train.shape, y1_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "(1600, 640, 128) (1600,)\n"
     ]
    }
   ],
   "source": [
    "X2_train, y2_train = setup_model_data(df2_train)\n",
    "np.savez('train2_arr', X2_train, y2_train)\n",
    "print(X2_train.shape, y2_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Couldn't process:  812\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "(1599, 640, 128) (1600,)\n"
     ]
    }
   ],
   "source": [
    "X4_train, y4_train = setup_model_data(df4_train)\n",
    "np.savez('train4_arr', X4_train, y4_train)\n",
    "print(X4_train.shape, y4_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Couldn't process:  296\n",
      "Couldn't process:  297\n",
      "Couldn't process:  298\n",
      "Currently processing:  300\n",
      "Couldn't process:  331\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Couldn't process:  698\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "(1595, 640, 128) (1600,)\n"
     ]
    }
   ],
   "source": [
    "X3_train, y3_train = setup_model_data(df3_train)\n",
    "np.savez('train3_arr', X3_train, y3_train)\n",
    "print(X3_train.shape, y3_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 640, 128) (800,)\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load('mel_valid_data.npz')\n",
    "X_valid = npzfile['arr_0']\n",
    "y_valid = npzfile['arr_1']\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6394, 640, 128) (6400,)\n",
      "-80.0 3.814697265625e-06 -48.55885427922595\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cf3045f7b3fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_valid' is not defined"
     ]
    }
   ],
   "source": [
    "npzfile1 = np.load('train1_arr.npz')\n",
    "npzfile2 = np.load('train2_arr.npz')\n",
    "npzfile3 = np.load('train3_arr.npz')\n",
    "npzfile4 = np.load('train4_arr.npz')\n",
    "X_train1 = npzfile1['arr_0']\n",
    "y_train1 = npzfile1['arr_1']\n",
    "X_train2 = npzfile2['arr_0']\n",
    "y_train2 = npzfile2['arr_1']\n",
    "X_train3 = npzfile3['arr_0']\n",
    "y_train3 = npzfile3['arr_1']\n",
    "X_train4 = npzfile4['arr_0']\n",
    "y_train4 = npzfile4['arr_1']\n",
    "\n",
    "X_train = np.concatenate((X_train1, X_train2, X_train3, X_train4), axis = 0)\n",
    "y_train = np.concatenate((y_train1, y_train2, y_train3, y_train4), axis = 0)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-80.0 3.814697265625e-06 -48.55885427922595\n",
      "-1 6 2.5\n"
     ]
    }
   ],
   "source": [
    "print(np.amin(X_train), np.amax(X_train), np.mean(X_train))\n",
    "y_train = y_train -1\n",
    "y_valid = y_valid -1\n",
    "print(np.amin(y_train), np.amax(y_train), np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amin(y_valid), np.amax(y_valid), np.mean(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = librosa.core.db_to_power(X_train, ref=1.0)\n",
    "print(np.amin(X_train_raw), np.amax(X_train_raw), np.mean(X_train_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_log = np.log(X_train_raw)\n",
    "print(np.amin(X_train_log), np.amax(X_train_log), np.mean(X_train_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_raw = librosa.core.db_to_power(X_valid, ref=1.0)\n",
    "X_valid_log = np.log(X_valid_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
