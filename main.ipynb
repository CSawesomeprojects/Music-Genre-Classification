{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython import display\n",
    "\n",
    "AUDIO_PATH = 'dataset/FMA/fma_small/'\n",
    "METADATA_PATH = 'dataset/FMA/fma_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple wrapper class for (1-channel) audio data\n",
    "# data is a 1-D NumPy array containing the data\n",
    "# rate is a number expressing the samples per second\n",
    "# == Modified from 554X class example code ==\n",
    "class Audio:\n",
    "    def __init__(self, data, rate, fn):\n",
    "        self.data = data\n",
    "        self.rate = rate\n",
    "        self.filename = fn.split(\"/\")[-1]\n",
    "    def play(self):\n",
    "        return display.Audio(self.data, rate=self.rate)\n",
    "    def plot_wave(self):\n",
    "        librosa.display.waveplot(self.data, sr=self.rate)\n",
    "    def create_spectrum(self, n_fft, hop_length):\n",
    "        # n_fft = int(self.rate / 20)\n",
    "        # hop_length = n_fft / 4\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(self.data, n_fft=n_fft)), ref=np.max)\n",
    "        return D\n",
    "    def create_melspectrum(self, n_fft, hop_length):\n",
    "        D = librosa.power_to_db(librosa.feature.melspectrogram(self.data, sr=self.rate, n_fft=n_fft, hop_length=hop_length), ref=np.max)\n",
    "        return D\n",
    "    def plot_spectrum(self, D, y_axis, hop_length):\n",
    "        librosa.display.specshow(D, y_axis=y_axis, x_axis='time', sr=self.rate, hop_length=hop_length)\n",
    "    @classmethod\n",
    "    def fromfile(cls, fn):\n",
    "        return cls(*librosa.load(fn, sr=None), fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tids_from_directory(audio_dir):\n",
    "    \"\"\"Get track IDs from the mp3s in a directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_dir : str\n",
    "        Path to the directory where the audio files are stored.\n",
    "    Returns\n",
    "    -------\n",
    "        A list of track IDs.\n",
    "    \"\"\"\n",
    "    tids = []\n",
    "    for _, dirnames, files in os.walk(audio_dir):\n",
    "        if dirnames == []:\n",
    "            tids.extend(int(file[:-4]) for file in files)\n",
    "    return tids\n",
    "\n",
    "\n",
    "def get_audio_path(audio_dir, track_id):\n",
    "    \"\"\"\n",
    "    Return the path to the mp3 given the directory where the audio is stored\n",
    "    and the track ID.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import utils\n",
    "    >>> AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    >>> utils.get_audio_path(AUDIO_DIR, 2)\n",
    "    '../data/fma_small/000/000002.mp3'\n",
    "    \"\"\"\n",
    "    tid_str = '{:06d}'.format(track_id)\n",
    "    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "dataset/FMA/fma_small/135/135054.mp3\n"
     ]
    }
   ],
   "source": [
    "tids = get_tids_from_directory(AUDIO_PATH)\n",
    "print(len(tids))\n",
    "print(get_audio_path(AUDIO_PATH, tids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 4)\n",
      "{'Hip-Hop': 1, 'Pop': 2, 'Folk': 3, 'Experimental': 4, 'Rock': 5, 'International': 6, 'Electronic': 7, 'Instrumental': 8}\n"
     ]
    }
   ],
   "source": [
    "# Load genres and metadata\n",
    "tracks = pd.read_csv(os.path.join(METADATA_PATH, \"tracks.csv\"), index_col=0, header=[0, 1])\n",
    "keep_cols = [('set', 'split'), ('set', 'subset'), ('track', 'genre_top')]\n",
    "\n",
    "df_all = tracks[keep_cols]\n",
    "df_all = df_all[df_all[('set', 'subset')] == 'small'] # only extract FMA_small metadata\n",
    "df_all['track_id'] = df_all.index\n",
    "print(df_all.shape)\n",
    "\n",
    "# Create dictionary of genres from unique genre labels\n",
    "unique_genres = df_all[('track', 'genre_top')].unique()\n",
    "dict_genres = { unique_genres[i] : i+1 for i in range(0, len(unique_genres)) } # i+1 because feels weird to have 0 as label\n",
    "print(dict_genres)\n",
    "\n",
    "# df_all.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                set            track track_id\n",
      "             split subset genre_top         \n",
      "track_id                                    \n",
      "2         training  small   Hip-Hop        2\n",
      "5         training  small   Hip-Hop        5\n",
      "10        training  small       Pop       10\n",
      "140       training  small      Folk      140\n",
      "141       training  small      Folk      141\n",
      "...            ...    ...       ...      ...\n",
      "154308        test  small   Hip-Hop   154308\n",
      "154309        test  small   Hip-Hop   154309\n",
      "154413    training  small       Pop   154413\n",
      "154414    training  small       Pop   154414\n",
      "155066    training  small   Hip-Hop   155066\n",
      "\n",
      "[8000 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process data into spectrogram and genre labels for model\n",
    "def setup_model_data(df):\n",
    "    genres = []\n",
    "    X_spect = np.empty((0, 640, 128))\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            count += 1\n",
    "            tid = int(row['track_id'])\n",
    "            genre = str(row[('track', 'genre_top')])\n",
    "            genres.append(dict_genres[genre])\n",
    "            \n",
    "            audio = Audio.fromfile(get_audio_path(AUDIO_PATH, tid))\n",
    "            spect = audio.create_melspectrum(2048, 1024)\n",
    "            spect = spect.T[:640, :]\n",
    "            X_spect = np.append(X_spect, [spect], axis=0)\n",
    "            if count % 100 == 0:\n",
    "                print(\"Currently processing: \", count)\n",
    "        except:\n",
    "            print(\"Couldn't process: \", count)\n",
    "            continue\n",
    "    y_labels = np.array(genres)\n",
    "    return X_spect, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split full data into train, validation, test data frames\n",
    "df_train = df_all[df_all[('set', 'split')]=='training']\n",
    "df_valid = df_all[df_all[('set', 'split')]=='validation']\n",
    "df_test = df_all[df_all[('set', 'split')]=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               set                track track_id\n",
      "             split subset     genre_top         \n",
      "track_id                                        \n",
      "2         training  small       Hip-Hop        2\n",
      "5         training  small       Hip-Hop        5\n",
      "10        training  small           Pop       10\n",
      "140       training  small          Folk      140\n",
      "141       training  small          Folk      141\n",
      "...            ...    ...           ...      ...\n",
      "55402     training  small          Rock    55402\n",
      "55430     training  small           Pop    55430\n",
      "55436     training  small           Pop    55436\n",
      "55437     training  small           Pop    55437\n",
      "55480     training  small  Experimental    55480\n",
      "\n",
      "[1800 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train.head(1800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONLY RUN AND UNCOMMENT THIS IF YOU NEED TO PROCESS TRAIN DATA AGAIN. \n",
    "\n",
    "# X_test, y_test = setup_model_data(df_test)\n",
    "# np.savez('test_data', X_test, y_test)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "(800, 640, 128) (800,)\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN AND UNCOMMENT THIS IF YOU NEED TO PROCESS VALIDATION DATA AGAIN.\n",
    "\n",
    "# X_valid, y_valid = setup_model_data(df_valid)\n",
    "# np.savez('valid_data', X_valid, y_valid)\n",
    "# print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "Currently processing:  1700\n",
      "Currently processing:  1800\n",
      "Currently processing:  1900\n",
      "Currently processing:  2000\n",
      "Currently processing:  2100\n",
      "Currently processing:  2200\n",
      "Currently processing:  2300\n",
      "Currently processing:  2400\n",
      "Currently processing:  2500\n",
      "Currently processing:  2600\n",
      "Currently processing:  2700\n",
      "Currently processing:  2800\n",
      "Currently processing:  2900\n",
      "Currently processing:  3000\n",
      "Currently processing:  3100\n",
      "Currently processing:  3200\n",
      "Couldn't process:  3265\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN AND UNCOMMENT THIS IF YOU NEED TO PROCESS TRAINING DATA AGAIN.\n",
    "# WARNING: TAKES A VERY LONG TIME!!!\n",
    "\n",
    "# X_train, y_train = setup_model_data(df_train)\n",
    "# np.savez('train_arr', X_train, y_train)\n",
    "# print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 4) (1600, 4) (1600, 4) (1600, 4)\n"
     ]
    }
   ],
   "source": [
    "# Batch training data because it's too big to be processed all at once\n",
    "\n",
    "def splitDataFrameIntoSmaller(df, chunkSize = 1600): \n",
    "    listOfDf = list()\n",
    "    numberChunks = len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n",
    "    return listOfDf\n",
    "\n",
    "listDf = splitDataFrameIntoSmaller(df_train)\n",
    "df1_train = listDf[0]\n",
    "df2_train = listDf[1]\n",
    "df3_train = listDf[2]\n",
    "df4_train = listDf[3]\n",
    "print(df1_train.shape, df2_train.shape, df3_train.shape, df4_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dbea363d2f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_model_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train1_arr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1_train' is not defined"
     ]
    }
   ],
   "source": [
    "X1_train, y1_train = setup_model_data(df1_train)\n",
    "np.savez('train1_arr', X1_train, y1_train)\n",
    "print(X1_train.shape, y1_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "(1600, 640, 128) (1600,)\n"
     ]
    }
   ],
   "source": [
    "X2_train, y2_train = setup_model_data(df2_train)\n",
    "np.savez('train2_arr', X2_train, y2_train)\n",
    "print(X2_train.shape, y2_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Couldn't process:  812\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "(1599, 640, 128) (1600,)\n"
     ]
    }
   ],
   "source": [
    "X4_train, y4_train = setup_model_data(df4_train)\n",
    "np.savez('train4_arr', X4_train, y4_train)\n",
    "print(X4_train.shape, y4_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Couldn't process:  296\n",
      "Couldn't process:  297\n",
      "Couldn't process:  298\n",
      "Currently processing:  300\n",
      "Couldn't process:  331\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Couldn't process:  698\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n",
      "Currently processing:  1100\n",
      "Currently processing:  1200\n",
      "Currently processing:  1300\n",
      "Currently processing:  1400\n",
      "Currently processing:  1500\n",
      "Currently processing:  1600\n",
      "(1595, 640, 128) (1600,)\n"
     ]
    }
   ],
   "source": [
    "X3_train, y3_train = setup_model_data(df3_train)\n",
    "np.savez('train3_arr', X3_train, y3_train)\n",
    "print(X3_train.shape, y3_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 640, 128) (800,)\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load('mel_valid_data.npz')\n",
    "X_valid = npzfile['arr_0']\n",
    "y_valid = npzfile['arr_1']\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 640, 128) (800,)\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load('mel_test_data.npz')\n",
    "X_test = npzfile['arr_0']\n",
    "y_test = npzfile['arr_1']\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile1 = np.load('train1_arr.npz')\n",
    "npzfile2 = np.load('train2_arr.npz')\n",
    "npzfile3 = np.load('train3_arr.npz')\n",
    "npzfile4 = np.load('train4_arr.npz')\n",
    "X_train1 = npzfile1['arr_0']\n",
    "y_train1 = npzfile1['arr_1']\n",
    "X_train2 = npzfile2['arr_0']\n",
    "y_train2 = npzfile2['arr_1']\n",
    "X_train3 = npzfile3['arr_0']\n",
    "y_train3 = npzfile3['arr_1']\n",
    "X_train4 = npzfile4['arr_0']\n",
    "y_train4 = npzfile4['arr_1']\n",
    "\n",
    "X_train = np.concatenate((X_train1, X_train2, X_train3, X_train4), axis = 0)\n",
    "y_train = np.concatenate((y_train1, y_train2, y_train3, y_train4), axis = 0)\n",
    "np.savez('mel_train_data.npz', X_train, y_train)\n",
    "\n",
    "# print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to get subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    244\n",
       "6    242\n",
       "1    212\n",
       "4    205\n",
       "2    204\n",
       "3    198\n",
       "5    175\n",
       "8    120\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.Index(y_train2)\n",
    "index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    100\n",
       "7    100\n",
       "6    100\n",
       "5    100\n",
       "4    100\n",
       "3    100\n",
       "2    100\n",
       "1    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.Index(y_valid)\n",
    "index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    100\n",
       "7    100\n",
       "6    100\n",
       "5    100\n",
       "4    100\n",
       "3    100\n",
       "2    100\n",
       "1    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.Index(y_test)\n",
    "index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-80.0 3.814697265625e-06 -48.55885427922595\n",
      "-1 6 2.5\n"
     ]
    }
   ],
   "source": [
    "print(np.amin(X_train), np.amax(X_train), np.mean(X_train))\n",
    "y_train = y_train -1\n",
    "y_valid = y_valid -1\n",
    "print(np.amin(y_train), np.amax(y_train), np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amin(y_valid), np.amax(y_valid), np.mean(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = librosa.core.db_to_power(X_train, ref=1.0)\n",
    "print(np.amin(X_train_raw), np.amax(X_train_raw), np.mean(X_train_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_log = np.log(X_train_raw)\n",
    "print(np.amin(X_train_log), np.amax(X_train_log), np.mean(X_train_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_raw = librosa.core.db_to_power(X_valid, ref=1.0)\n",
    "X_valid_log = np.log(X_valid_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with just a subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 640, 128)\n",
      "(1600,)\n",
      "(800, 640, 128)\n",
      "(800,)\n",
      "(800, 640, 128)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train2.shape)\n",
    "print(y_train2.shape)\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
