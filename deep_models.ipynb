{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm, sklearn.ensemble\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import scipy\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(train_path, val_path, test_path):\n",
    "    npz_train = np.load(train_path)\n",
    "    npz_val = np.load(val_path)\n",
    "    npz_test = np.load(test_path)\n",
    "    \n",
    "    X_train = npz_train['arr_0']\n",
    "    y_train = npz_train['arr_1']\n",
    "    X_val = npz_val['arr_0']\n",
    "    y_val = npz_val['arr_1']\n",
    "    X_test = npz_test['arr_0']\n",
    "    y_test = npz_test['arr_1']\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test \n",
    "\n",
    "def encode_labels(y_train, y_val, y_test):\n",
    "    # Convert label data into one-hot encoding for softmax\n",
    "    le = skl.preprocessing.OneHotEncoder(sparse=False)\n",
    "    y_train = le.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_val = le.fit_transform(y_val.reshape(-1, 1))\n",
    "    y_test = le.fit_transform(y_test.reshape(-1, 1))\n",
    "    return y_train, y_val, y_test\n",
    "\n",
    "def scale_shuffle_data(X, y): \n",
    "    X_raw = librosa.core.db_to_power(X, ref=1.0)\n",
    "    X_log = np.log(X_raw)\n",
    "    X, y = skl.utils.shuffle(X_log, y)\n",
    "    return X, y\n",
    "\n",
    "def scale_shuffle_train_data(X, y): \n",
    "    X_raw = librosa.core.db_to_power(X, ref=1.0)\n",
    "    X_log = np.log(X_raw)\n",
    "    X, y = unison_shuffled_copies(X_log, y)\n",
    "    return X, y\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_processed_data(\n",
    "    'train2_arr.npz', 'mel_valid_data.npz', 'mel_test_data.npz')\n",
    "y_train, y_val, y_test = encode_labels(y_train, y_val, y_test)\n",
    "X_train, y_train = scale_shuffle_data(X_train, y_train)\n",
    "X_val, y_val = scale_shuffle_data(X_val, y_val)\n",
    "X_test, y_test = scale_shuffle_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 640, 128)\n",
      "(1600, 8)\n",
      "(800, 640, 128)\n",
      "(800, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Conv2D, concatenate, MaxPooling2D, Flatten, Embedding, Lambda\n",
    "from keras.layers import Input, Dense, Bidirectional, LSTM, Dropout, Activation, GRU\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "num_classes = 8\n",
    "n_frames = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "nb_filters1 = 16 \n",
    "nb_filters2 = 32 \n",
    "nb_filters3 = 64\n",
    "nb_filters4 = 64\n",
    "nb_filters5 = 64\n",
    "ksize = (3,1)\n",
    "pool_size_1 = (2,2) \n",
    "pool_size_2 = (4,4)\n",
    "pool_size_3 = (4,2)\n",
    "\n",
    "dropout_prob = 0.20\n",
    "dense_size1 = 128\n",
    "lstm_count = 64\n",
    "num_units = 120\n",
    "\n",
    "def build_pcrnn_model(input_layer):\n",
    "    print('Building parallel RNN+CNN model...')\n",
    "    \n",
    "    # Set up convolutional layers\n",
    "    conv_1 = Conv2D(filters = nb_filters1, kernel_size = ksize, \n",
    "                    strides = 1, padding = 'valid', activation = 'relu', \n",
    "                    name = 'conv_1')(input_layer)\n",
    "    pool_1 = MaxPooling2D(pool_size_1)(conv_1)\n",
    "    \n",
    "    conv_2 = Conv2D(filters = nb_filters2, kernel_size = ksize, \n",
    "                    strides = 1, padding = 'valid', activation = 'relu', \n",
    "                    name = 'conv_2')(pool_1)\n",
    "    pool_2 = MaxPooling2D(pool_size_1)(conv_2)\n",
    "    \n",
    "    conv_3 = Conv2D(filters = nb_filters3, kernel_size = ksize, \n",
    "                    strides = 1, padding = 'valid', activation = 'relu', \n",
    "                    name = 'conv_3')(pool_2)\n",
    "    pool_3 = MaxPooling2D(pool_size_1)(conv_3)\n",
    "    \n",
    "    conv_4 = Conv2D(filters = nb_filters4, kernel_size = ksize, \n",
    "                    strides = 1, padding = 'valid', activation = 'relu', \n",
    "                    name = 'conv_4')(pool_3)\n",
    "    pool_4 = MaxPooling2D(pool_size_2)(conv_4)\n",
    "    \n",
    "    conv_5 = Conv2D(filters = nb_filters5, kernel_size = ksize, \n",
    "                    strides = 1, padding = 'valid', activation = 'relu', \n",
    "                    name = 'conv_5')(pool_4)\n",
    "    pool_5 = MaxPooling2D(pool_size_2)(conv_5)\n",
    "    \n",
    "    flatten1 = Flatten()(pool_5)\n",
    "    \n",
    "    \n",
    "    # Set up recurrent layers\n",
    "    pool_lstm1 = MaxPooling2D(pool_size_3, name = 'pool_lstm')(input_layer)\n",
    "    \n",
    "    squeezed = Lambda(lambda x: K.squeeze(x, axis = -1))(pool_lstm1)\n",
    "    \n",
    "    lstm = Bidirectional(GRU(lstm_count))(squeezed)\n",
    "    \n",
    "    \n",
    "    # Concatenate output of CNN and RNN \n",
    "    combined = concatenate([flatten1, lstm], axis=-1, name='combined')\n",
    "    \n",
    "    # Softmax\n",
    "    output = Dense(num_classes, activation = 'softmax', name = 'softmax')(combined)\n",
    "    model = Model(input_layer, output)\n",
    "    \n",
    "    # Some kind of optimizer..?\n",
    "    opt = RMSprop(lr = 0.005) \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def train(X_train, y_train, X_val, y_val, name):\n",
    "    X_train = np.expand_dims(X_train, axis = -1)\n",
    "    X_val = np.expand_dims(X_val, axis = -1)\n",
    "    \n",
    "    input_layer = Input((n_frames, n_features, 1))\n",
    "    model = build_pcrnn_model(input_layer)\n",
    "    \n",
    "    # Checkpoints..?\n",
    "    checkpoint_callback = ModelCheckpoint('./models/prcnn/' + name, monitor ='val_accuracy', verbose = 1,\n",
    "                                          save_best_only = True, mode = 'max')\n",
    "    \n",
    "    reducelr_callback = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.5, patience = 10, \n",
    "                                          min_delta = 0.01, verbose = 1)\n",
    "    callbacks_list = [checkpoint_callback, reducelr_callback]\n",
    "    \n",
    "    print('Training the parallel RNN+CNN model...')\n",
    "    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs,\n",
    "                       validation_data = (X_val, y_val), verbose = 1, callbacks = callbacks_list)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building parallel RNN+CNN model...\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 640, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 638, 128, 16) 64          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling2D) (None, 319, 64, 16)  0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 317, 64, 32)  1568        max_pooling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling2D) (None, 158, 32, 32)  0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 156, 32, 64)  6208        max_pooling2d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling2D) (None, 78, 16, 64)   0           conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 76, 16, 64)   12352       max_pooling2d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling2D) (None, 19, 4, 64)    0           conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 17, 4, 64)    12352       max_pooling2d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool_lstm (MaxPooling2D)        (None, 160, 64, 1)   0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling2D) (None, 4, 1, 64)     0           conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 160, 64)      0           pool_lstm[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 256)          0           max_pooling2d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 128)          49536       lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "combined (Concatenate)          (None, 384)          0           flatten_8[0][0]                  \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 8)            3080        combined[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 85,160\n",
      "Trainable params: 85,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Training the parallel RNN+CNN model...\n",
      "Train on 1600 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "1600/1600 [==============================] - 71s 44ms/step - loss: 2.4324 - accuracy: 0.1350 - val_loss: 2.2071 - val_accuracy: 0.1250\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.12500, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 2/50\n",
      "1600/1600 [==============================] - 62s 39ms/step - loss: 2.1409 - accuracy: 0.1394 - val_loss: 2.1341 - val_accuracy: 0.1238\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.12500\n",
      "Epoch 3/50\n",
      "1600/1600 [==============================] - 64s 40ms/step - loss: 2.1373 - accuracy: 0.1256 - val_loss: 2.1664 - val_accuracy: 0.1262\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.12500 to 0.12625, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 4/50\n",
      "1600/1600 [==============================] - 66s 41ms/step - loss: 2.1251 - accuracy: 0.1394 - val_loss: 2.0503 - val_accuracy: 0.1412\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.12625 to 0.14125, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 5/50\n",
      "1600/1600 [==============================] - 61s 38ms/step - loss: 2.1838 - accuracy: 0.1488 - val_loss: 2.1204 - val_accuracy: 0.1825\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.14125 to 0.18250, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 6/50\n",
      "1600/1600 [==============================] - 68s 42ms/step - loss: 2.0714 - accuracy: 0.1706 - val_loss: 2.0226 - val_accuracy: 0.1762\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.18250\n",
      "Epoch 7/50\n",
      "1600/1600 [==============================] - 60s 37ms/step - loss: 2.0888 - accuracy: 0.1725 - val_loss: 2.0266 - val_accuracy: 0.1600\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.18250\n",
      "Epoch 8/50\n",
      "1600/1600 [==============================] - 69s 43ms/step - loss: 2.0412 - accuracy: 0.1950 - val_loss: 2.1073 - val_accuracy: 0.1400\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.18250\n",
      "Epoch 9/50\n",
      "1600/1600 [==============================] - 56s 35ms/step - loss: 2.0658 - accuracy: 0.1825 - val_loss: 2.0384 - val_accuracy: 0.1675\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.18250\n",
      "Epoch 10/50\n",
      "1600/1600 [==============================] - 57s 35ms/step - loss: 1.9965 - accuracy: 0.2144 - val_loss: 2.1831 - val_accuracy: 0.1762\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.18250\n",
      "Epoch 11/50\n",
      "1600/1600 [==============================] - 54s 34ms/step - loss: 1.9927 - accuracy: 0.2113 - val_loss: 1.9770 - val_accuracy: 0.2275\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.18250 to 0.22750, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 12/50\n",
      "1600/1600 [==============================] - 53s 33ms/step - loss: 1.9527 - accuracy: 0.2606 - val_loss: 2.0482 - val_accuracy: 0.2313\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.22750 to 0.23125, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 13/50\n",
      "1600/1600 [==============================] - 48s 30ms/step - loss: 1.8922 - accuracy: 0.2775 - val_loss: 2.2896 - val_accuracy: 0.2125\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.23125\n",
      "Epoch 14/50\n",
      "1600/1600 [==============================] - 60s 37ms/step - loss: 1.8724 - accuracy: 0.2681 - val_loss: 2.1828 - val_accuracy: 0.2100\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.23125\n",
      "Epoch 15/50\n",
      "1600/1600 [==============================] - 50s 31ms/step - loss: 1.8490 - accuracy: 0.3094 - val_loss: 2.1018 - val_accuracy: 0.2037\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.23125\n",
      "Epoch 16/50\n",
      "1600/1600 [==============================] - 57s 36ms/step - loss: 1.7845 - accuracy: 0.3356 - val_loss: 2.1508 - val_accuracy: 0.2050\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.23125\n",
      "Epoch 17/50\n",
      "1600/1600 [==============================] - 64s 40ms/step - loss: 1.6702 - accuracy: 0.3688 - val_loss: 2.2458 - val_accuracy: 0.2275\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.23125\n",
      "Epoch 18/50\n",
      "1600/1600 [==============================] - 68s 43ms/step - loss: 1.6627 - accuracy: 0.3713 - val_loss: 2.1234 - val_accuracy: 0.2688\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.23125 to 0.26875, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 19/50\n",
      "1600/1600 [==============================] - 59s 37ms/step - loss: 1.5471 - accuracy: 0.4369 - val_loss: 2.3120 - val_accuracy: 0.2475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.26875\n",
      "Epoch 20/50\n",
      "1600/1600 [==============================] - 64s 40ms/step - loss: 1.5390 - accuracy: 0.4269 - val_loss: 2.1906 - val_accuracy: 0.2600\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.26875\n",
      "Epoch 21/50\n",
      "1600/1600 [==============================] - 70s 43ms/step - loss: 1.4370 - accuracy: 0.4762 - val_loss: 2.5159 - val_accuracy: 0.2400\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.26875\n",
      "Epoch 22/50\n",
      "1600/1600 [==============================] - 54s 34ms/step - loss: 1.3393 - accuracy: 0.5063 - val_loss: 2.5265 - val_accuracy: 0.2488\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.26875\n",
      "Epoch 23/50\n",
      "1600/1600 [==============================] - 62s 39ms/step - loss: 1.2182 - accuracy: 0.5550 - val_loss: 2.6574 - val_accuracy: 0.2713\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.26875 to 0.27125, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 24/50\n",
      "1600/1600 [==============================] - 66s 41ms/step - loss: 1.2875 - accuracy: 0.5437 - val_loss: 2.5522 - val_accuracy: 0.2637\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.27125\n",
      "Epoch 25/50\n",
      "1600/1600 [==============================] - 58s 37ms/step - loss: 1.0915 - accuracy: 0.6056 - val_loss: 2.8070 - val_accuracy: 0.2550\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.27125\n",
      "Epoch 26/50\n",
      "1600/1600 [==============================] - 65s 41ms/step - loss: 1.0303 - accuracy: 0.6250 - val_loss: 2.8317 - val_accuracy: 0.2750\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.27125 to 0.27500, saving model to ./models/prcnn/weights.best.h5\n",
      "Epoch 27/50\n",
      "1600/1600 [==============================] - 64s 40ms/step - loss: 0.9120 - accuracy: 0.6894 - val_loss: 3.7797 - val_accuracy: 0.2050\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.27500\n",
      "Epoch 28/50\n",
      "1600/1600 [==============================] - 65s 40ms/step - loss: 0.8801 - accuracy: 0.7044 - val_loss: 3.2709 - val_accuracy: 0.2562\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.27500\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 29/50\n",
      "1600/1600 [==============================] - 74s 46ms/step - loss: 0.4633 - accuracy: 0.8569 - val_loss: 3.6198 - val_accuracy: 0.2700\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.27500\n",
      "Epoch 30/50\n",
      "1600/1600 [==============================] - 61s 38ms/step - loss: 0.4039 - accuracy: 0.8731 - val_loss: 3.8496 - val_accuracy: 0.2512\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.27500\n",
      "Epoch 31/50\n",
      "1600/1600 [==============================] - 67s 42ms/step - loss: 0.3299 - accuracy: 0.8906 - val_loss: 4.1615 - val_accuracy: 0.2475\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.27500\n",
      "Epoch 32/50\n",
      "1600/1600 [==============================] - 66s 41ms/step - loss: 0.3006 - accuracy: 0.9100 - val_loss: 4.2721 - val_accuracy: 0.2512\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.27500\n",
      "Epoch 33/50\n",
      "1600/1600 [==============================] - 72s 45ms/step - loss: 0.2502 - accuracy: 0.9225 - val_loss: 4.5860 - val_accuracy: 0.2700\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.27500\n",
      "Epoch 34/50\n",
      "1600/1600 [==============================] - 74s 46ms/step - loss: 0.1788 - accuracy: 0.9588 - val_loss: 5.1489 - val_accuracy: 0.2475\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.27500\n",
      "Epoch 35/50\n",
      "1600/1600 [==============================] - 86s 54ms/step - loss: 0.1860 - accuracy: 0.9425 - val_loss: 5.5178 - val_accuracy: 0.2412\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.27500\n",
      "Epoch 36/50\n",
      "1600/1600 [==============================] - 66s 41ms/step - loss: 0.1348 - accuracy: 0.9663 - val_loss: 5.5289 - val_accuracy: 0.2512\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.27500\n",
      "Epoch 37/50\n",
      "1600/1600 [==============================] - 106s 66ms/step - loss: 0.2542 - accuracy: 0.9275 - val_loss: 5.8912 - val_accuracy: 0.2438\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.27500\n",
      "Epoch 38/50\n",
      "1600/1600 [==============================] - 126s 79ms/step - loss: 0.0669 - accuracy: 0.9869 - val_loss: 5.7508 - val_accuracy: 0.2725\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.27500\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 39/50\n",
      "1600/1600 [==============================] - 95s 60ms/step - loss: 0.0480 - accuracy: 0.9937 - val_loss: 6.4244 - val_accuracy: 0.2463\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.27500\n",
      "Epoch 40/50\n",
      "1600/1600 [==============================] - 77s 48ms/step - loss: 0.0426 - accuracy: 0.9912 - val_loss: 6.4835 - val_accuracy: 0.2700\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.27500\n",
      "Epoch 41/50\n",
      "1600/1600 [==============================] - 80s 50ms/step - loss: 0.0350 - accuracy: 0.9956 - val_loss: 7.0274 - val_accuracy: 0.2713\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.27500\n",
      "Epoch 42/50\n",
      "1600/1600 [==============================] - 67s 42ms/step - loss: 0.0274 - accuracy: 0.9962 - val_loss: 7.2477 - val_accuracy: 0.2587\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.27500\n",
      "Epoch 43/50\n",
      "1600/1600 [==============================] - 57s 36ms/step - loss: 0.0379 - accuracy: 0.9906 - val_loss: 7.6850 - val_accuracy: 0.2537\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.27500\n",
      "Epoch 44/50\n",
      "1600/1600 [==============================] - 57s 36ms/step - loss: 0.0232 - accuracy: 0.9969 - val_loss: 7.8073 - val_accuracy: 0.2625\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.27500\n",
      "Epoch 45/50\n",
      "1600/1600 [==============================] - 62s 39ms/step - loss: 0.0184 - accuracy: 0.9981 - val_loss: 8.0254 - val_accuracy: 0.2587\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.27500\n",
      "Epoch 46/50\n",
      "1600/1600 [==============================] - 86s 54ms/step - loss: 0.0350 - accuracy: 0.9944 - val_loss: 8.0130 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.27500\n",
      "Epoch 47/50\n",
      "1600/1600 [==============================] - 68s 43ms/step - loss: 0.0230 - accuracy: 0.9950 - val_loss: 8.3697 - val_accuracy: 0.2587\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.27500\n",
      "Epoch 48/50\n",
      "1600/1600 [==============================] - 58s 36ms/step - loss: 0.0204 - accuracy: 0.9975 - val_loss: 8.5989 - val_accuracy: 0.2550\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.27500\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 49/50\n",
      "1600/1600 [==============================] - 2127s 1s/step - loss: 0.0150 - accuracy: 0.9981 - val_loss: 8.5715 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.27500\n",
      "Epoch 50/50\n",
      "1600/1600 [==============================] - 51s 32ms/step - loss: 0.0124 - accuracy: 0.9981 - val_loss: 8.7842 - val_accuracy: 0.2525\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.27500\n"
     ]
    }
   ],
   "source": [
    "model, history  = train(X_train, y_train, X_val, y_val, 'full-weights.best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 'models/prcnn/base-weights.best.h5'\n",
    "model = load_model(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing base test data:\n",
    "base_npzfile = np.load('base_test_arr.npz')\n",
    "base_X_test = base_npzfile['arr_0']\n",
    "base_y_test = base_npzfile['arr_1']\n",
    "# base_X_test, base_y_test = scale_shuffle_data(base_X_test, base_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.420680743952367 1.554312234475218e-15 -10.044188292533065\n"
     ]
    }
   ],
   "source": [
    "X_test_raw = librosa.core.db_to_power(base_X_test, ref=1.0)\n",
    "X_test_raw = np.log(X_test_raw)\n",
    "print(np.amin(X_test_raw), np.amax(X_test_raw), np.mean(X_test_raw))\n",
    "\n",
    "X_test_exp = np.expand_dims(X_test_raw, axis = -1)\n",
    "y_pred = model.predict(X_test_exp)\n",
    "y_pred = np.argmax(y_pred, axis = 1)\n",
    "y_true = base_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.51      0.55       100\n",
      "           1       0.30      0.30      0.30       100\n",
      "           2       0.24      0.28      0.26       100\n",
      "           3       0.63      0.80      0.70       100\n",
      "           4       0.43      0.41      0.42       100\n",
      "           5       0.58      0.45      0.51       100\n",
      "           6       0.26      0.19      0.22       100\n",
      "           7       0.49      0.61      0.54       100\n",
      "\n",
      "    accuracy                           0.44       800\n",
      "   macro avg       0.44      0.44      0.44       800\n",
      "weighted avg       0.44      0.44      0.44       800\n",
      "\n",
      "0.44375\n"
     ]
    }
   ],
   "source": [
    "y_true = y_true - 1\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing own models.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 'models/prcnn/weights.best.h5'\n",
    "model = load_model(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_exp = np.expand_dims(X_test, axis = -1)\n",
    "y_pred = model.predict(X_test_exp)\n",
    "y_pred = np.argmax(y_pred, axis = 1)\n",
    "y_true = np.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.33      0.28       100\n",
      "           1       0.21      0.12      0.15       100\n",
      "           2       0.11      0.06      0.08       100\n",
      "           3       0.22      0.21      0.22       100\n",
      "           4       0.40      0.38      0.39       100\n",
      "           5       0.20      0.23      0.21       100\n",
      "           6       0.24      0.18      0.21       100\n",
      "           7       0.28      0.46      0.35       100\n",
      "\n",
      "    accuracy                           0.25       800\n",
      "   macro avg       0.24      0.25      0.23       800\n",
      "weighted avg       0.24      0.25      0.23       800\n",
      "\n",
      "0.24625\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(path):\n",
    "    tracks = pd.read_csv(os.path.join(METADATA_PATH, \"tracks.csv\"), index_col=0, header=[0, 1])\n",
    "    keep_cols = [('set', 'split'), ('set', 'subset'), ('track', 'genre_top')]\n",
    "    tracks = tracks[keep_cols]\n",
    "    \n",
    "    features = pd.read_csv(os.path.join(METADATA_PATH, \"features.csv\"), index_col=0, header=[0, 1, 2], skip_blank_lines=True)\n",
    "    return tracks, features\n",
    "\n",
    "def setup_data(tracks, features):\n",
    "    small = tracks['set', 'subset'] == 'small'\n",
    "    \n",
    "    train = tracks['set', 'split'] == 'training'\n",
    "    val = tracks['set', 'split'] == 'validation'\n",
    "    test = tracks['set', 'split'] == 'test'\n",
    "    \n",
    "    y_train = tracks.loc[small & train, ('track', 'genre_top')]\n",
    "    y_val = tracks.loc[small & val, ('track', 'genre_top')]\n",
    "    y_test = tracks.loc[small & test, ('track', 'genre_top')]\n",
    "\n",
    "    X_train = features.loc[small & train, 'mfcc']\n",
    "    X_val = features.loc[small & val, 'mfcc']\n",
    "    X_test = features.loc[small & test, 'mfcc']\n",
    "    \n",
    "    # Shuffle training data\n",
    "    X_train, y_train = skl.utils.shuffle(X_train, y_train)\n",
    "    \n",
    "    # Standardize features - remove mean and scale accordingly\n",
    "    standardize = skl.preprocessing.StandardScaler(copy=False)\n",
    "    X_train = standardize.fit_transform(X_train)\n",
    "    X_val = standardize.fit_transform(X_val)\n",
    "    X_test = standardize.fit_transform(X_test)\n",
    "    \n",
    "    # Label encode outputs\n",
    "    le = skl.preprocessing.LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_val = le.fit_transform(y_val)\n",
    "    y_test = le.fit_transform(y_test)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def train(c, X_train, y_train):\n",
    "    c.fit(X_train, y_train)\n",
    "\n",
    "def predict(c, X_test):\n",
    "    y_pred = c.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_PATH = 'dataset/FMA/fma_metadata/'\n",
    "tracks, features = load_metadata(METADATA_PATH)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = setup_data(tracks, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC - F1 scores:\n",
      "Train: 0.7581\n",
      "Valid: 0.5450\n",
      "Test: 0.4650\n"
     ]
    }
   ],
   "source": [
    "c_svc = skl.svm.SVC()\n",
    "train(c_svc, X_train, y_train)\n",
    "y_pred_train = predict(c_svc, X_train)\n",
    "y_pred_val = predict(c_svc, X_val)\n",
    "y_pred_test_svc = predict(c_svc, X_test)\n",
    "print(\"SVC - F1 scores:\")\n",
    "print(\"Train: {:.4f}\".format(f1_score(y_train, y_pred_train, average='micro', pos_label=1)))\n",
    "print(\"Valid: {:.4f}\".format(f1_score(y_val, y_pred_val, average='micro', pos_label=1)))\n",
    "print(\"Test: {:.4f}\".format(f1_score(y_test, y_pred_test_svc, average='micro', pos_label=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests - F1 scores:\n",
      "Train: 0.9269\n",
      "Valid: 0.5275\n",
      "Test: 0.4387\n"
     ]
    }
   ],
   "source": [
    "c_rf = skl.ensemble.RandomForestClassifier(random_state=10, max_depth=30, n_estimators=300, min_samples_leaf=6, min_impurity_decrease=0.0002, \n",
    "                                           class_weight='balanced')\n",
    "train(c_rf, X_train, y_train)\n",
    "y_pred_train = predict(c_rf, X_train)\n",
    "y_pred_val = predict(c_rf, X_val)\n",
    "y_pred_test_rf = predict(c_rf, X_test)\n",
    "print(\"Random Forests - F1 scores:\")\n",
    "print(\"Train: {:.4f}\".format(f1_score(y_train, y_pred_train, average='micro', pos_label=1)))\n",
    "print(\"Valid: {:.4f}\".format(f1_score(y_val, y_pred_val, average='micro', pos_label=1)))\n",
    "print(\"Test: {:.4f}\".format(f1_score(y_test, y_pred_test_rf, average='micro', pos_label=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44375\n",
      "0.465\n",
      "0.43875\n",
      "0.46625\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "prcnn = y_pred\n",
    "svc = y_pred_test_svc\n",
    "rf = y_pred_test_rf\n",
    "labels.append(prcnn)\n",
    "labels.append(svc)\n",
    "labels.append(rf)\n",
    "labels = np.array(labels)\n",
    "labels = np.transpose(labels, (1, 0))\n",
    "labels = scipy.stats.mode(labels, axis=1)[0]\n",
    "labels = np.squeeze(labels)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "print(accuracy_score(y_true, y_pred_test_svc))\n",
    "print(accuracy_score(y_true, y_pred_test_rf))\n",
    "print(accuracy_score(y_true, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
